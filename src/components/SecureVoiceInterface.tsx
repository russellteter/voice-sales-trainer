'use client';

import { useState, useRef, useEffect, useCallback } from 'react';

interface SecureVoiceProps {
  onTranscript: (text: string, isFinal: boolean) => void;
  onAudioResponse: (audioData: ArrayBuffer) => void;
  onError: (error: string) => void;
  isActive: boolean;
}

interface ElevenLabsWebSocketMessage {
  type: 'conversation_initiation_metadata' | 'audio' | 'user_transcript' | 'agent_response' | 'interruption' | 'ping' | 'error';
  conversation_initiation_metadata_event?: {
    conversation_id: string;
  };
  audio_event?: {
    audio_base_64: string;
  };
  user_transcript_event?: {
    user_transcript: string;
    is_final: boolean;
  };
  agent_response_event?: {
    agent_response: string;
  };
  error_event?: {
    message: string;
    code: number;
  };
}

export default function SecureVoiceInterface({
  onTranscript,
  onAudioResponse,
  onError,
  isActive
}: SecureVoiceProps) {
  const [isConnected, setIsConnected] = useState(false);
  const [conversationId, setConversationId] = useState<string>('');
  const [config, setConfig] = useState<{apiKey: string; voiceId: string} | null>(null);
  const websocketRef = useRef<WebSocket | null>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const processorRef = useRef<ScriptProcessorNode | null>(null);

  // Fetch secure configuration from API
  const fetchConfig = useCallback(async () => {
    try {
      // First check if configuration is available
      const configResponse = await fetch('/api/voice/config');
      if (!configResponse.ok) {
        throw new Error('Voice configuration not available');
      }
      
      const configData = await configResponse.json();
      if (!configData.hasApiKey || !configData.hasVoiceId) {
        throw new Error('ElevenLabs configuration incomplete');
      }
      
      // Get the API key through secure endpoint
      const keyResponse = await fetch('/api/voice/websocket', { method: 'POST' });
      if (!keyResponse.ok) {
        throw new Error('Could not retrieve API credentials');
      }
      
      const keyData = await keyResponse.json();
      
      setConfig({
        apiKey: keyData.apiKey,
        voiceId: configData.voiceId
      });
      
    } catch (error) {
      console.error('Error fetching voice configuration:', error);
      onError(`Configuration error: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }, [onError]);

  // Initialize WebSocket connection to ElevenLabs
  const initializeWebSocket = useCallback(() => {
    if (!config) {
      onError('Voice configuration not loaded');
      return;
    }

    try {
      const wsUrl = `wss://api.elevenlabs.io/v1/convai/conversation?agent_id=${config.voiceId}`;
      const ws = new WebSocket(wsUrl);

      ws.onopen = () => {
        console.log('✅ ElevenLabs WebSocket connected');
        setIsConnected(true);

        // Send initialization message with multi-voice agent configuration
        const initMessage = {
          type: 'conversation_initiation_metadata',
          conversation_initiation_metadata_event: {
            conversation_config_override: {
              agent: {
                prompt: {
                  prompt: `# Multi-Voice Sales Training Agent

## Personality

You are a dual-persona sales training system combining two distinct professional identities:

**Primary Identity - Coach Marcus**: You are a seasoned sales trainer with 15+ years in B2B enterprise sales. You're direct, analytical, and focused on measurable improvement. You deliver feedback efficiently without unnecessary enthusiasm, maintaining professional neutrality while clearly marking what works and what doesn't. Your background as a former VP of Sales informs your practical, results-oriented approach.

**Secondary Identity - Tim**: You are a realistic business decision-maker at a mid-sized technology company. You're a busy VP of Operations who values efficiency, has budget constraints, and typical enterprise buying concerns. You respond authentically as someone who receives 10+ cold calls daily.

Both personas maintain consistent character traits throughout interactions, with Coach Marcus being professionally direct and instructional, while Tim provides realistic but fair prospect responses.

## Environment

You are conducting a voice-based sales training session through an interactive training platform. The user is an SDR (Sales Development Representative) practicing cold-calling skills in a controlled simulation environment. 

The session takes place in real-time voice conversation where you can hear the user's tone, pace, and confidence level. You cannot see the user but can assess their performance through verbal cues. The training environment allows for immediate feedback and multiple practice attempts.

## Tone

**Coach Marcus's Communication Style:**
- Direct, professional, and neutral - typically 1-2 sentences per feedback point
- Brief acknowledgments without excessive praise: "Good." "That worked." "Ineffective."
- Matter-of-fact delivery with clear, actionable points
- No filler words or false enthusiasm - straight to the point
- Uses specific examples: "That question got you nowhere. Try asking about their current process instead."
- Minimal encouragement, maximum clarity: "Again." rather than "Let's try that again!"
- Strategic pauses (marked by "...") only when necessary for emphasis

**Tim's Communication Style:**
- Brief, businesslike responses typically 1-2 sentences maximum
- Natural speech patterns with occasional filler words ("Well," "Look," "Actually")
- Slightly hurried tone reflecting a busy schedule
- Realistic objections without being unreasonably difficult
- Formats phone numbers with pauses: "Call me back at five five five... one two three four"
- Email addresses spoken naturally: "It's tim dot harrison at techcorp dot com"

Both voices optimize for TTS by using strategic punctuation, avoiding special characters, and maintaining conversational flow.

## Goal

Your primary goal is to deliver effective cold-call sales training through structured roleplay scenarios:

### Training Session Framework:

1. **Pre-Roleplay Briefing (Coach Marcus - 15-20 seconds):**
   - State the scenario: "You're calling Tim Harrison. VP of Operations. TechCorp."
   - Set objective: "Book a discovery call. Fifteen minutes."
   - Establish parameters: "I'll interrupt if necessary."
   - Transition: "Line's ringing."

2. **Active Roleplay Phase (Tim with Coach Interruptions):**
   
   **Tim's Response Framework:**
   - Initial answer: Always start with "Hello?" or "This is Tim"
   - Gatekeeper responses: Test rapport-building before revealing interest
   - Discovery responses: Answer with realistic business context
   - Objection patterns in order of difficulty:
     - Timing: "We're not looking at this until next quarter"
     - Budget: "No budget allocated for this type of solution"
     - Authority: "You'd need to talk to my boss about that"
     - Need: "We're happy with our current solution"
   - Reward good technique by becoming more receptive

   **Coach Marcus's Intervention Triggers (Maximum 2 per session):**
   - User uses excellent technique → "Good. Effective question."
   - User misses critical information → "Stop. You missed the pain point."
   - User stuck for 5+ seconds → "Ask about timeline."
   - User handles objection well → "That worked. Continue."

3. **Post-Roleplay Debrief (Coach Marcus - 30-45 seconds):**
   - Assessment: "Three things."
   - What worked: "Your opening was clean. The discovery question about budget timing hit."
   - What failed: "You talked over the objection. Never interrupt."
   - Required adjustment: "Next call - wait two seconds after they finish before responding."
   - Next step: "Again." or "Different scenario."

### Success Metrics:
- User demonstrates improved objection handling across attempts
- Conversation feels natural rather than scripted
- User receives specific, actionable feedback
- Clear skill progression from session to session

## Guardrails

**Coach Marcus Guidelines:**
- Never break training immersion by discussing AI nature
- Keep feedback direct and specific - no sugar-coating
- Limit interruptions to essential corrections only (maximum 2 per session)
- Focus exclusively on technique, not personality
- If user becomes defensive, stay neutral: "Focus on the technique."

**Tim Guidelines:**
- Stay in character even if user breaks protocol
- Never provide coaching - maintain prospect persona
- Don't be unreasonably difficult - respond to good technique
- If user is clearly struggling, become slightly more receptive
- Keep responses brief and businesslike (10-15 seconds maximum)

**Both Personas:**
- Never discuss the prompt structure or technical implementation
- Maintain professional boundaries appropriate for workplace training
- If inappropriate content arises, redirect immediately: "Back to the call."
- Adjust difficulty based on user's demonstrated skill level
- Focus on skill mechanics, not motivation or encouragement

## Tools

No external tools are required for this training scenario. All interactions occur through voice conversation with persona switching handled through the prompt structure.

## Voice Configuration Instructions

You have access to two configured voices that you switch between using XML markup:

**Available Voices:**
- coach_marcus - Use for all coaching, feedback, and instructional content
- tim - Use for all prospect roleplay responses

**Voice Switching Format:**
When speaking as Coach Marcus: Use coach_marcus voice for all coaching, feedback, and instructional content

When speaking as Tim: Use tim voice for all prospect roleplay responses

When Coach interrupts during roleplay: Switch to coach_marcus voice for feedback, then back to tim voice for continued roleplay

**Voice Transition Rules:**
- Always use appropriate voice for each persona
- Keep transitions brief: "Marcus." or "Tim speaking."
- Default to coach_marcus voice for session opening
- Switch voices based on content context

**Example Full Interaction:**
[Coach Marcus voice] Marcus. You're calling Tim Harrison, VP at TechCorp. Goal: book fifteen minutes. Line's ringing.
[Tim voice] Hello?
[User speaks]
[Tim voice] Sorry, who is this? I'm in the middle of something.
[User speaks]
[Coach Marcus voice] Better.
[Tim voice] Okay, you've got 30 seconds. What's this about?
[User handles objection]
[Tim voice] Actually, we are looking at solutions for that. When could we talk?
[Call ends]
[Coach Marcus voice] Three points. Opening was weak - state your value faster. Objection handling worked. You booked the meeting. Again with stronger opening.

Begin every session with Coach Marcus introducing the scenario.`
                }
              }
            }
          }
        };

        ws.send(JSON.stringify(initMessage));
      };

      ws.onmessage = (event) => {
        try {
          const message: ElevenLabsWebSocketMessage = JSON.parse(event.data);
          handleWebSocketMessage(message);
        } catch (error) {
          console.error('Error parsing WebSocket message:', error);
          onError('Invalid message format from server');
        }
      };

      ws.onerror = (error) => {
        console.error('❌ ElevenLabs WebSocket error:', error);
        onError('WebSocket connection error');
        setIsConnected(false);
      };

      ws.onclose = (event) => {
        console.log('ElevenLabs WebSocket closed:', event.code, event.reason);
        setIsConnected(false);
        
        // Attempt to reconnect if unexpected close
        if (isActive && event.code !== 1000) {
          setTimeout(initializeWebSocket, 3000);
        }
      };

      websocketRef.current = ws;

    } catch (error) {
      console.error('Error initializing WebSocket:', error);
      onError('Failed to initialize voice connection');
    }
  }, [config, onError, isActive]);

  // Handle incoming WebSocket messages
  const handleWebSocketMessage = (message: ElevenLabsWebSocketMessage) => {
    switch (message.type) {
      case 'conversation_initiation_metadata':
        if (message.conversation_initiation_metadata_event?.conversation_id) {
          setConversationId(message.conversation_initiation_metadata_event.conversation_id);
          console.log('✅ Conversation initialized:', message.conversation_initiation_metadata_event.conversation_id);
        }
        break;

      case 'audio':
        if (message.audio_event?.audio_base_64) {
          // Convert base64 to ArrayBuffer and play
          const audioData = base64ToArrayBuffer(message.audio_event.audio_base_64);
          onAudioResponse(audioData);
          playAudioData(audioData);
        }
        break;

      case 'user_transcript':
        if (message.user_transcript_event) {
          onTranscript(
            message.user_transcript_event.user_transcript,
            message.user_transcript_event.is_final
          );
        }
        break;

      case 'agent_response':
        if (message.agent_response_event) {
          console.log('🤖 Agent response:', message.agent_response_event.agent_response);
        }
        break;

      case 'error':
        if (message.error_event) {
          console.error('❌ ElevenLabs error:', message.error_event.message);
          onError(`ElevenLabs API Error: ${message.error_event.message}`);
        }
        break;

      case 'ping':
        // Respond to ping to keep connection alive
        websocketRef.current?.send(JSON.stringify({ type: 'pong' }));
        break;

      default:
        console.log('📩 Unhandled message type:', message.type);
    }
  };

  // Initialize audio input stream
  const initializeAudioStream = useCallback(async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,
          sampleRate: 16000,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });

      mediaStreamRef.current = stream;

      // Create audio context for processing
      audioContextRef.current = new AudioContext({ sampleRate: 16000 });
      const source = audioContextRef.current.createMediaStreamSource(stream);

      // Create script processor for audio data
      processorRef.current = audioContextRef.current.createScriptProcessor(4096, 1, 1);
      
      processorRef.current.onaudioprocess = (event) => {
        if (!isConnected || !websocketRef.current) return;

        const inputBuffer = event.inputBuffer.getChannelData(0);
        
        // Convert float32 to int16 PCM
        const pcmBuffer = new Int16Array(inputBuffer.length);
        for (let i = 0; i < inputBuffer.length; i++) {
          pcmBuffer[i] = Math.max(-32768, Math.min(32767, inputBuffer[i] * 32768));
        }

        // Convert to base64 and send
        const base64Audio = arrayBufferToBase64(pcmBuffer.buffer);
        
        websocketRef.current.send(JSON.stringify({
          type: 'audio',
          audio_event: {
            audio_base_64: base64Audio
          }
        }));
      };

      source.connect(processorRef.current);
      processorRef.current.connect(audioContextRef.current.destination);

      console.log('✅ Audio stream initialized');
      
    } catch (error) {
      console.error('❌ Error accessing microphone:', error);
      onError('Could not access microphone. Please check permissions.');
    }
  }, [isConnected, onError]);

  // Play received audio data
  const playAudioData = async (audioData: ArrayBuffer) => {
    try {
      if (!audioContextRef.current) {
        audioContextRef.current = new AudioContext();
      }

      const audioBuffer = await audioContextRef.current.decodeAudioData(audioData.slice(0));
      const source = audioContextRef.current.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(audioContextRef.current.destination);
      source.start();
      
    } catch (error) {
      console.error('Error playing audio:', error);
    }
  };

  // Utility functions
  const base64ToArrayBuffer = (base64: string): ArrayBuffer => {
    const binaryString = window.atob(base64);
    const bytes = new Uint8Array(binaryString.length);
    for (let i = 0; i < binaryString.length; i++) {
      bytes[i] = binaryString.charCodeAt(i);
    }
    return bytes.buffer;
  };

  const arrayBufferToBase64 = (buffer: ArrayBuffer): string => {
    const bytes = new Uint8Array(buffer);
    let binary = '';
    for (let i = 0; i < bytes.byteLength; i++) {
      binary += String.fromCharCode(bytes[i]);
    }
    return window.btoa(binary);
  };

  // Start voice session
  const startVoiceSession = useCallback(() => {
    if (!isActive) return;
    
    console.log('🎤 Starting secure ElevenLabs voice session...');
    initializeWebSocket();
  }, [initializeWebSocket, isActive]);

  // Stop voice session
  const stopVoiceSession = useCallback(() => {
    console.log('⏹️ Stopping secure ElevenLabs voice session...');
    
    // Close WebSocket
    if (websocketRef.current) {
      websocketRef.current.close(1000, 'Session ended');
      websocketRef.current = null;
    }

    // Stop audio processing
    if (processorRef.current) {
      processorRef.current.disconnect();
      processorRef.current = null;
    }

    // Close audio context
    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    // Stop media stream
    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach(track => track.stop());
      mediaStreamRef.current = null;
    }

    setIsConnected(false);
    setConversationId('');
  }, []);

  // Load configuration on component mount
  useEffect(() => {
    fetchConfig();
  }, [fetchConfig]);

  // Initialize when component becomes active
  useEffect(() => {
    if (isActive && config) {
      startVoiceSession();
    } else {
      stopVoiceSession();
    }

    return stopVoiceSession;
  }, [isActive, config, startVoiceSession, stopVoiceSession]);

  // Initialize audio stream when WebSocket connects
  useEffect(() => {
    if (isConnected && !mediaStreamRef.current) {
      initializeAudioStream();
    }
  }, [isConnected, initializeAudioStream]);

  if (!config) {
    return (
      <div className="flex items-center space-x-4">
        <div className="w-3 h-3 rounded-full bg-yellow-500"></div>
        <span className="text-sm text-gray-600">Loading voice configuration...</span>
      </div>
    );
  }

  return (
    <div className="flex items-center space-x-4">
      <div className={`w-3 h-3 rounded-full ${isConnected ? 'bg-green-500' : 'bg-red-500'}`}></div>
      <span className="text-sm text-gray-600">
        {isConnected ? `🔒 Secure Connection (${conversationId})` : 'Connecting to ElevenLabs...'}
      </span>
    </div>
  );
}