'use client';

import { useState, useRef, useEffect, useCallback } from 'react';

interface ElevenLabsVoiceProps {
  apiKey: string;
  voiceId: string;
  onTranscript: (text: string, isFinal: boolean) => void;
  onAudioResponse: (audioData: ArrayBuffer) => void;
  onError: (error: string) => void;
  isActive: boolean;
}

interface ElevenLabsWebSocketMessage {
  type: 'conversation_initiation_metadata' | 'audio' | 'user_transcript' | 'agent_response' | 'interruption' | 'ping' | 'error';
  conversation_initiation_metadata_event?: {
    conversation_id: string;
  };
  audio_event?: {
    audio_base_64: string;
  };
  user_transcript_event?: {
    user_transcript: string;
    is_final: boolean;
  };
  agent_response_event?: {
    agent_response: string;
  };
  error_event?: {
    message: string;
    code: number;
  };
}

export default function ElevenLabsVoiceInterface({
  apiKey,
  voiceId,
  onTranscript,
  onAudioResponse,
  onError,
  isActive
}: ElevenLabsVoiceProps) {
  const [isConnected, setIsConnected] = useState(false);
  const [conversationId, setConversationId] = useState<string>('');
  const [reconnectAttempts, setReconnectAttempts] = useState(0);
  const [lastPingTime, setLastPingTime] = useState<number>(0);
  const [latency, setLatency] = useState<number | undefined>(undefined);
  const [audioQuality, setAudioQuality] = useState<'excellent' | 'good' | 'fair' | 'poor' | 'unknown'>('unknown');
  const websocketRef = useRef<WebSocket | null>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const processorRef = useRef<AudioWorkletNode | null>(null);
  const audioWorkletLoaded = useRef<boolean>(false);
  const reconnectTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const maxReconnectAttempts = 5;
  const baseReconnectDelay = 1000; // 1 second base delay

  // Initialize WebSocket connection to ElevenLabs
  const initializeWebSocket = useCallback(() => {
    if (!apiKey || !voiceId) {
      onError('Missing API key or Voice ID');
      return;
    }

    try {
      const wsUrl = `wss://api.elevenlabs.io/v1/convai/conversation?agent_id=${voiceId}`;
      const ws = new WebSocket(wsUrl, [], {
        headers: {
          'xi-api-key': apiKey
        }
      });

      ws.onopen = () => {
        console.log('âœ… ElevenLabs WebSocket connected');
        setIsConnected(true);
        setReconnectAttempts(0); // Reset reconnection attempts on successful connection

        // Send initialization message with multi-voice agent configuration
        const initMessage = {
          type: 'conversation_initiation_metadata',
          conversation_initiation_metadata_event: {
            conversation_config_override: {
              agent: {
                prompt: {
                  prompt: `# Multi-Voice Sales Training Agent

## Personality

You are a dual-persona sales training system combining two distinct professional identities:

**Primary Identity - Coach Marcus**: You are a seasoned sales trainer with 15+ years in B2B enterprise sales. You're direct, analytical, and focused on measurable improvement. You deliver feedback efficiently without unnecessary enthusiasm, maintaining professional neutrality while clearly marking what works and what doesn't. Your background as a former VP of Sales informs your practical, results-oriented approach.

**Secondary Identity - Tim**: You are a realistic business decision-maker at a mid-sized technology company. You're a busy VP of Operations who values efficiency, has budget constraints, and typical enterprise buying concerns. You respond authentically as someone who receives 10+ cold calls daily.

Both personas maintain consistent character traits throughout interactions, with Coach Marcus being professionally direct and instructional, while Tim provides realistic but fair prospect responses.

## Environment

You are conducting a voice-based sales training session through an interactive training platform. The user is an SDR (Sales Development Representative) practicing cold-calling skills in a controlled simulation environment. 

The session takes place in real-time voice conversation where you can hear the user's tone, pace, and confidence level. You cannot see the user but can assess their performance through verbal cues. The training environment allows for immediate feedback and multiple practice attempts.

## Tone

**Coach Marcus's Communication Style:**
- Direct, professional, and neutral - typically 1-2 sentences per feedback point
- Brief acknowledgments without excessive praise: "Good." "That worked." "Ineffective."
- Matter-of-fact delivery with clear, actionable points
- No filler words or false enthusiasm - straight to the point
- Uses specific examples: "That question got you nowhere. Try asking about their current process instead."
- Minimal encouragement, maximum clarity: "Again." rather than "Let's try that again!"
- Strategic pauses (marked by "...") only when necessary for emphasis

**Tim's Communication Style:**
- Brief, businesslike responses typically 1-2 sentences maximum
- Natural speech patterns with occasional filler words ("Well," "Look," "Actually")
- Slightly hurried tone reflecting a busy schedule
- Realistic objections without being unreasonably difficult
- Formats phone numbers with pauses: "Call me back at five five five... one two three four"
- Email addresses spoken naturally: "It's tim dot harrison at techcorp dot com"

Both voices optimize for TTS by using strategic punctuation, avoiding special characters, and maintaining conversational flow.

## Goal

Your primary goal is to deliver effective cold-call sales training through structured roleplay scenarios:

### Training Session Framework:

1. **Pre-Roleplay Briefing (Coach Marcus - 15-20 seconds):**
   - State the scenario: "You're calling Tim Harrison. VP of Operations. TechCorp."
   - Set objective: "Book a discovery call. Fifteen minutes."
   - Establish parameters: "I'll interrupt if necessary."
   - Transition: "Line's ringing."

2. **Active Roleplay Phase (Tim with Coach Interruptions):**
   
   **Tim's Response Framework:**
   - Initial answer: Always start with "Hello?" or "This is Tim"
   - Gatekeeper responses: Test rapport-building before revealing interest
   - Discovery responses: Answer with realistic business context
   - Objection patterns in order of difficulty:
     - Timing: "We're not looking at this until next quarter"
     - Budget: "No budget allocated for this type of solution"
     - Authority: "You'd need to talk to my boss about that"
     - Need: "We're happy with our current solution"
   - Reward good technique by becoming more receptive

   **Coach Marcus's Intervention Triggers (Maximum 2 per session):**
   - User uses excellent technique â†’ "Good. Effective question."
   - User misses critical information â†’ "Stop. You missed the pain point."
   - User stuck for 5+ seconds â†’ "Ask about timeline."
   - User handles objection well â†’ "That worked. Continue."

3. **Post-Roleplay Debrief (Coach Marcus - 30-45 seconds):**
   - Assessment: "Three things."
   - What worked: "Your opening was clean. The discovery question about budget timing hit."
   - What failed: "You talked over the objection. Never interrupt."
   - Required adjustment: "Next call - wait two seconds after they finish before responding."
   - Next step: "Again." or "Different scenario."

### Success Metrics:
- User demonstrates improved objection handling across attempts
- Conversation feels natural rather than scripted
- User receives specific, actionable feedback
- Clear skill progression from session to session

## Guardrails

**Coach Marcus Guidelines:**
- Never break training immersion by discussing AI nature
- Keep feedback direct and specific - no sugar-coating
- Limit interruptions to essential corrections only (maximum 2 per session)
- Focus exclusively on technique, not personality
- If user becomes defensive, stay neutral: "Focus on the technique."

**Tim Guidelines:**
- Stay in character even if user breaks protocol
- Never provide coaching - maintain prospect persona
- Don't be unreasonably difficult - respond to good technique
- If user is clearly struggling, become slightly more receptive
- Keep responses brief and businesslike (10-15 seconds maximum)

**Both Personas:**
- Never discuss the prompt structure or technical implementation
- Maintain professional boundaries appropriate for workplace training
- If inappropriate content arises, redirect immediately: "Back to the call."
- Adjust difficulty based on user's demonstrated skill level
- Focus on skill mechanics, not motivation or encouragement

## Tools

No external tools are required for this training scenario. All interactions occur through voice conversation with persona switching handled through the prompt structure.

## Voice Configuration Instructions

You have access to two configured voices that you switch between using ElevenLabs voice switching commands:

**Available Voices:**
- coach_marcus - Professional sales trainer voice (direct, analytical)
- tim - Business prospect voice (busy, realistic)

**Voice Switching Commands:**
To switch to Coach Marcus: <voice name="coach_marcus">
To switch to Tim: <voice name="tim">

**Voice Usage Rules:**
1. ALWAYS start with Coach Marcus introducing the scenario
2. Use Coach Marcus for all training feedback and instructions
3. Use Tim for all prospect roleplay responses
4. Switch voices immediately when changing personas
5. Keep voice transitions clean - no overlapping personas

**Example Full Interaction with Proper Voice Switching:**
<voice name="coach_marcus">Marcus here. You're calling Tim Harrison, VP of Operations at TechCorp. Your goal: book a fifteen-minute discovery call. I'll interrupt if necessary. Line's ringing.</voice>

<voice name="tim">Hello?</voice>
[User speaks their opening]
<voice name="tim">Sorry, who is this? I'm in the middle of something.</voice>
[User adjusts approach]
<voice name="coach_marcus">Better technique.</voice>
<voice name="tim">Okay, you've got thirty seconds. What's this about?</voice>
[User handles objection]
<voice name="tim">Actually, we are looking at solutions for that. When could we talk?</voice>
[Call ends]
<voice name="coach_marcus">Three points. Opening was weak - state your value faster. Objection handling worked. You booked the meeting. Try again with a stronger opening.</voice>

**Critical Requirements:**
- Begin every session with: <voice name="coach_marcus">
- End voice commands properly with </voice>
- Never speak as both personas simultaneously
- Switch voices based on content context, not time
- Use Coach Marcus for session control and Tim for prospect responses`
                }
              }
            }
          }
        };

        ws.send(JSON.stringify(initMessage));
      };

      ws.onmessage = (event) => {
        try {
          const message: ElevenLabsWebSocketMessage = JSON.parse(event.data);
          handleWebSocketMessage(message);
        } catch (error) {
          console.error('Error parsing WebSocket message:', error);
          onError('Invalid message format from server');
        }
      };

      ws.onerror = (error) => {
        console.error('âŒ ElevenLabs WebSocket error:', error);
        onError('WebSocket connection error');
        setIsConnected(false);
      };

      ws.onclose = (event) => {
        console.log('ElevenLabs WebSocket closed:', event.code, event.reason);
        setIsConnected(false);
        
        // Clear any existing reconnect timeout
        if (reconnectTimeoutRef.current) {
          clearTimeout(reconnectTimeoutRef.current);
          reconnectTimeoutRef.current = null;
        }
        
        // Attempt to reconnect with exponential backoff if unexpected close
        if (isActive && event.code !== 1000 && reconnectAttempts < maxReconnectAttempts) {
          const delay = Math.min(baseReconnectDelay * Math.pow(2, reconnectAttempts), 30000); // Max 30 seconds
          console.log(`âš ï¸ Reconnecting in ${delay}ms (attempt ${reconnectAttempts + 1}/${maxReconnectAttempts})`);
          
          reconnectTimeoutRef.current = setTimeout(() => {
            setReconnectAttempts(prev => prev + 1);
            initializeWebSocket();
          }, delay);
        } else if (reconnectAttempts >= maxReconnectAttempts) {
          console.error('âŒ Max reconnection attempts reached');
          onError('Connection failed after multiple attempts. Please refresh the page.');
        }
      };

      websocketRef.current = ws;

    } catch (error) {
      console.error('Error initializing WebSocket:', error);
      onError('Failed to initialize voice connection');
    }
  }, [apiKey, voiceId, onError, isActive]);

  // Handle incoming WebSocket messages
  const handleWebSocketMessage = (message: ElevenLabsWebSocketMessage) => {
    switch (message.type) {
      case 'conversation_initiation_metadata':
        if (message.conversation_initiation_metadata_event?.conversation_id) {
          setConversationId(message.conversation_initiation_metadata_event.conversation_id);
          console.log('âœ… Conversation initialized:', message.conversation_initiation_metadata_event.conversation_id);
        }
        break;

      case 'audio':
        if (message.audio_event?.audio_base_64) {
          // Convert base64 to ArrayBuffer and play
          const audioData = base64ToArrayBuffer(message.audio_event.audio_base_64);
          onAudioResponse(audioData);
          playAudioData(audioData);
        }
        break;

      case 'user_transcript':
        if (message.user_transcript_event) {
          onTranscript(
            message.user_transcript_event.user_transcript,
            message.user_transcript_event.is_final
          );
        }
        break;

      case 'agent_response':
        if (message.agent_response_event) {
          console.log('ðŸ¤– Agent response:', message.agent_response_event.agent_response);
        }
        break;

      case 'error':
        if (message.error_event) {
          console.error('âŒ ElevenLabs error:', message.error_event.message);
          onError(`ElevenLabs API Error: ${message.error_event.message}`);
        }
        break;

      case 'ping':
        // Respond to ping to keep connection alive and measure latency
        const pingTime = Date.now();
        setLastPingTime(pingTime);
        websocketRef.current?.send(JSON.stringify({ type: 'pong' }));
        break;
        
      case 'pong':
        // Calculate latency from ping/pong
        if (lastPingTime > 0) {
          const currentLatency = Date.now() - lastPingTime;
          setLatency(currentLatency);
          
          // Update audio quality based on latency
          if (currentLatency < 100) {
            setAudioQuality('excellent');
          } else if (currentLatency < 200) {
            setAudioQuality('good');
          } else if (currentLatency < 500) {
            setAudioQuality('fair');
          } else {
            setAudioQuality('poor');
          }
        }
        break;

      default:
        console.log('ðŸ“© Unhandled message type:', message.type);
    }
  };

  // Initialize audio input stream
  const initializeAudioStream = useCallback(async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,
          sampleRate: 16000,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });

      mediaStreamRef.current = stream;

      // Create audio context for processing
      audioContextRef.current = new AudioContext({ sampleRate: 16000 });
      const source = audioContextRef.current.createMediaStreamSource(stream);

      // Use modern AudioWorkletNode for better performance
      if (!audioWorkletLoaded.current) {
        try {
          await audioContextRef.current.audioWorklet.addModule(
            'data:application/javascript,' + encodeURIComponent(`
              class AudioProcessor extends AudioWorkletProcessor {
                constructor() {
                  super();
                  this.bufferSize = 4096;
                  this.buffer = new Float32Array(this.bufferSize);
                  this.bufferIndex = 0;
                }
                
                process(inputs, outputs, parameters) {
                  const input = inputs[0];
                  if (input.length > 0) {
                    const inputData = input[0];
                    
                    for (let i = 0; i < inputData.length; i++) {
                      this.buffer[this.bufferIndex] = inputData[i];
                      this.bufferIndex++;
                      
                      if (this.bufferIndex >= this.bufferSize) {
                        // Convert to PCM and send
                        const pcmBuffer = new Int16Array(this.bufferSize);
                        for (let j = 0; j < this.bufferSize; j++) {
                          pcmBuffer[j] = Math.max(-32768, Math.min(32767, this.buffer[j] * 32768));
                        }
                        
                        this.port.postMessage(pcmBuffer.buffer);
                        this.bufferIndex = 0;
                      }
                    }
                  }
                  return true;
                }
              }
              
              registerProcessor('audio-processor', AudioProcessor);
            `)
          );
          audioWorkletLoaded.current = true;
        } catch (error) {
          console.warn('AudioWorklet not supported, falling back to ScriptProcessor:', error);
          // Fallback to ScriptProcessor for older browsers
          const processor = audioContextRef.current.createScriptProcessor(4096, 1, 1);
          processor.onaudioprocess = (event) => {
            if (!isConnected || !websocketRef.current) return;
            const inputBuffer = event.inputBuffer.getChannelData(0);
            const pcmBuffer = new Int16Array(inputBuffer.length);
            for (let i = 0; i < inputBuffer.length; i++) {
              pcmBuffer[i] = Math.max(-32768, Math.min(32767, inputBuffer[i] * 32768));
            }
            const base64Audio = arrayBufferToBase64(pcmBuffer.buffer);
            websocketRef.current.send(JSON.stringify({
              type: 'audio',
              audio_event: { audio_base_64: base64Audio }
            }));
          };
          source.connect(processor);
          processor.connect(audioContextRef.current.destination);
          return;
        }
      }
      
      processorRef.current = new AudioWorkletNode(audioContextRef.current, 'audio-processor');
      
      processorRef.current.port.onmessage = (event) => {
        if (!isConnected || !websocketRef.current) return;
        
        const base64Audio = arrayBufferToBase64(event.data);
        websocketRef.current.send(JSON.stringify({
          type: 'audio',
          audio_event: {
            audio_base_64: base64Audio
          }
        }));
      };
      
      source.connect(processorRef.current);
      processorRef.current.connect(audioContextRef.current.destination);

      console.log('âœ… Audio stream initialized');
      
    } catch (error) {
      console.error('âŒ Error accessing microphone:', error);
      onError('Could not access microphone. Please check permissions.');
    }
  }, [isConnected, onError]);

  // Play received audio data with improved error handling
  const playAudioData = async (audioData: ArrayBuffer) => {
    try {
      if (!audioContextRef.current) {
        audioContextRef.current = new AudioContext({ sampleRate: 48000 });
      }
      
      // Resume context if suspended (required by browser autoplay policies)
      if (audioContextRef.current.state === 'suspended') {
        await audioContextRef.current.resume();
        console.log('AudioContext resumed for playback');
      }
      
      // Validate audio data
      if (!audioData || audioData.byteLength === 0) {
        console.warn('Empty audio data received, skipping playback');
        return;
      }

      const audioBuffer = await audioContextRef.current.decodeAudioData(audioData.slice(0));
      
      // Check if audio buffer is valid
      if (!audioBuffer || audioBuffer.length === 0) {
        console.warn('Invalid audio buffer, skipping playback');
        return;
      }
      
      const source = audioContextRef.current.createBufferSource();
      source.buffer = audioBuffer;
      
      // Add gain node for volume control
      const gainNode = audioContextRef.current.createGain();
      gainNode.gain.value = 0.8; // Slightly reduce volume to prevent clipping
      
      source.connect(gainNode);
      gainNode.connect(audioContextRef.current.destination);
      
      source.onerror = (error) => {
        console.error('Audio source error:', error);
      };
      
      source.start();
      
    } catch (error) {
      console.error('Error playing audio:', error);
      // Try to recover by creating a new audio context
      if (audioContextRef.current?.state === 'closed') {
        audioContextRef.current = new AudioContext({ sampleRate: 48000 });
        console.log('Audio context recreated after error');
      }
    }
  };

  // Utility functions
  const base64ToArrayBuffer = (base64: string): ArrayBuffer => {
    const binaryString = window.atob(base64);
    const bytes = new Uint8Array(binaryString.length);
    for (let i = 0; i < binaryString.length; i++) {
      bytes[i] = binaryString.charCodeAt(i);
    }
    return bytes.buffer;
  };

  const arrayBufferToBase64 = (buffer: ArrayBuffer): string => {
    const bytes = new Uint8Array(buffer);
    let binary = '';
    for (let i = 0; i < bytes.byteLength; i++) {
      binary += String.fromCharCode(bytes[i]);
    }
    return window.btoa(binary);
  };

  // Start voice session
  const startVoiceSession = useCallback(() => {
    if (!isActive) return;
    
    console.log('ðŸŽ¤ Starting ElevenLabs voice session...');
    initializeWebSocket();
  }, [initializeWebSocket, isActive]);

  // Stop voice session
  const stopVoiceSession = useCallback(() => {
    console.log('â¹ï¸ Stopping ElevenLabs voice session...');
    
    // Close WebSocket
    if (websocketRef.current) {
      websocketRef.current.close(1000, 'Session ended');
      websocketRef.current = null;
    }

    // Stop audio processing
    if (processorRef.current) {
      processorRef.current.disconnect();
      processorRef.current = null;
    }

    // Close audio context
    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    // Stop media stream
    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach(track => track.stop());
      mediaStreamRef.current = null;
    }

    setIsConnected(false);
    setConversationId('');
    setReconnectAttempts(0);
    setLatency(undefined);
    setAudioQuality('unknown');
    
    // Clear any pending reconnect timeouts
    if (reconnectTimeoutRef.current) {
      clearTimeout(reconnectTimeoutRef.current);
      reconnectTimeoutRef.current = null;
    }
  }, []);

  // Initialize when component becomes active
  useEffect(() => {
    if (isActive) {
      startVoiceSession();
    } else {
      stopVoiceSession();
    }

    return stopVoiceSession;
  }, [isActive, startVoiceSession, stopVoiceSession]);

  // Initialize audio stream when WebSocket connects
  useEffect(() => {
    if (isConnected && !mediaStreamRef.current) {
      initializeAudioStream();
    }
  }, [isConnected, initializeAudioStream]);

  return (
    <div className="flex items-center space-x-4">
      <div className={`w-3 h-3 rounded-full ${
        isConnected ? 'bg-green-500' : 
        reconnectAttempts > 0 ? 'bg-yellow-500 animate-pulse' : 
        'bg-red-500'
      }`}></div>
      <div className="flex flex-col">
        <span className="text-sm text-gray-600">
          {isConnected 
            ? `Connected (${conversationId.slice(-8) || 'N/A'})` 
            : reconnectAttempts > 0 
              ? `Reconnecting (${reconnectAttempts}/${maxReconnectAttempts})...` 
              : 'Connecting to ElevenLabs...'
          }
        </span>
        {(latency !== undefined || audioQuality !== 'unknown') && (
          <div className="flex items-center space-x-2 text-xs text-gray-500">
            {latency !== undefined && (
              <span className={`font-mono ${
                latency < 100 ? 'text-green-600' :
                latency < 200 ? 'text-yellow-600' :
                'text-red-600'
              }`}>
                {latency}ms
              </span>
            )}
            {audioQuality !== 'unknown' && (
              <span className={`capitalize ${
                audioQuality === 'excellent' ? 'text-green-600' :
                audioQuality === 'good' ? 'text-blue-600' :
                audioQuality === 'fair' ? 'text-yellow-600' :
                'text-red-600'
              }`}>
                {audioQuality}
              </span>
            )}
          </div>
        )}
      </div>
    </div>
  );
}

// Example usage hook
export function useElevenLabsVoice(apiKey: string, voiceId: string) {
  const [transcript, setTranscript] = useState<string>('');
  const [audioResponses, setAudioResponses] = useState<ArrayBuffer[]>([]);
  const [error, setError] = useState<string>('');
  const [isActive, setIsActive] = useState(false);

  const handleTranscript = useCallback((text: string, isFinal: boolean) => {
    setTranscript(prev => isFinal ? text : `${prev} ${text}`);
  }, []);

  const handleAudioResponse = useCallback((audioData: ArrayBuffer) => {
    setAudioResponses(prev => [...prev, audioData]);
  }, []);

  const handleError = useCallback((errorMessage: string) => {
    setError(errorMessage);
    console.error('ElevenLabs Voice Error:', errorMessage);
  }, []);

  const startSession = () => setIsActive(true);
  const stopSession = () => setIsActive(false);

  return {
    transcript,
    audioResponses,
    error,
    isActive,
    startSession,
    stopSession,
    VoiceInterface: () => (
      <ElevenLabsVoiceInterface
        apiKey={apiKey}
        voiceId={voiceId}
        onTranscript={handleTranscript}
        onAudioResponse={handleAudioResponse}
        onError={handleError}
        isActive={isActive}
      />
    )
  };
}